from inference.gemini import Gemini
from inference.gpt_seq import SequentialOpenAI
from inference.nop import nop_inference, pefect_inference
from inference.vllm import VLLM
import sys

triplets = sys.argv[1] # Folder containing the datasets
model_name = sys.argv[2] # Model to be evaluated
output_folder = sys.argv[3] # Folder to save images generated by LLM

vllm_params = {
  "max_tokens": 8192
}

# Inference
# Output should be a folder path containing the generated images
match model_name:
  case "gpt4o":
    gpt = SequentialOpenAI("gpt-4o-2024-08-06")
    output = gpt.inference(triplets, output_folder)
  case "gpt4o-mini":
    gpt = SequentialOpenAI("gpt-4o-mini-2024-07-18")
    output = gpt.inference(triplets, output_folder)
  case "gpt3.5":
    gpt = SequentialOpenAI("gpt-3.5-turbo-0125")
    output = gpt.inference(triplets, output_folder)
  case "o1":
    gpt = SequentialOpenAI("o1-preview-2024-09-12")
    output = gpt.inference(triplets, output_folder)
  case "o1-mini":
    gpt = SequentialOpenAI("o1-mini-2024-09-12")
    output = gpt.inference(triplets, output_folder)

  case "gemini-1.5-flash" | "gemini-1.5-pro" | "gemini-1.0-pro":
    gemini = Gemini(model_name)
    output = gemini.inference(triplets, output_folder)
  
  case "llama2-7b-chat":
    llama = VLLM("meta-llama/Llama-2-7b-chat-hf", **vllm_params)
    output = llama.inference(triplets, output_folder=output_folder)
  case "llama3-8b-instruct":
    llama = VLLM("meta-llama/Meta-Llama-3-8B-Instruct", **vllm_params)
    output = llama.inference(triplets, output_folder=output_folder)
  case "llama3.1-8b-instruct":
    llama = VLLM("meta-llama/Llama-3.1-8B-Instruct", **vllm_params)
    output = llama.inference(triplets, output_folder=output_folder)
  case "llama3.2-3b-instruct":
    llama = VLLM("meta-llama/Llama-3.2-3B-Instruct", **vllm_params)
    output = llama.inference(triplets, output_folder=output_folder)
  case "codellama-7b-instruct":
    llama = VLLM("meta-llama/CodeLlama-7b-Instruct-hf", llm_params={
      "tensor_parallel_size": 2,
      "max_seq_len_to_capture": 16384
    }, **vllm_params)
    output = llama.inference(triplets, output_folder)
  case "gemma1.1-7b-it":
    gemma = VLLM("google/gemma-1.1-7b-it", **vllm_params)
    output = gemma.inference(triplets, output_folder=output_folder)
  case "gemma2-2b-it":
    gemma = VLLM("google/gemma-2-2b-it", **vllm_params)
    output = gemma.inference(triplets, output_folder=output_folder)
  case "gemma2-9b-it":
    gemma = VLLM("google/gemma-2-9b-it", **vllm_params)
    output = gemma.inference(triplets, output_folder=output_folder)
  case "codegemma-7b-it":
    gemma = VLLM("google/codegemma-7b-it", **vllm_params)
    output = gemma.inference(triplets, output_folder=output_folder)
  case "phi3.5-mini-instruct":
    phi = VLLM("microsoft/Phi-3.5-mini-instruct", llm_params={
      "tensor_parallel_size": 2,
      "max_seq_len_to_capture": 131072
    }, **vllm_params)
    output = phi.inference(triplets, output_folder)
  case "phi3.5-vision-instruct":
    phi = VLLM("microsoft/Phi-3.5-vision-instruct", llm_params= {
      "tensor_parallel_size": 2,
      "max_seq_len_to_capture": 131072,
      "trust_remote_code": True
    }, **vllm_params)
    output = phi.inference(triplets, output_folder)
  case "mistral-7b-instruct-v0.2":
    mistral = VLLM("mistralai/Mistral-7B-Instruct-v0.2", **vllm_params)
    output = mistral.inference(triplets, output_folder=output_folder)
  case "mistral-7b-instruct-v0.3":
    mistral = VLLM("mistralai/Mistral-7B-Instruct-v0.3", **vllm_params)
    output = mistral.inference(triplets, output_folder=output_folder)
  case "llava-v1.6-mistral-7b":
    llava = VLLM("llava-hf/llava-v1.6-mistral-7b-hf", **vllm_params)
    output = llava.inference(triplets, output_folder=output_folder)
  case "qwen2-7b-instruct":
    qwen = VLLM("Qwen/Qwen2-VL-7B-Instruct", **vllm_params)
    output = qwen.inference(triplets, output_folder=output_folder)
  
  case "nop":
    output = nop_inference(triplets, output_folder)
  case "perfect":
    output = pefect_inference(triplets, output_folder)

  case _:
    raise ValueError("Unknown model name")

print(f"Results outputted to {output}")
